"""
æ¨¡å‹é€‚é…å™¨ - å¤„ç†ä¸åŒæ¨¡å‹çš„å‚æ•°å·®å¼‚

åŠŸèƒ½:
1. ç®¡ç†ä¸åŒæ¨¡å‹çš„å‚æ•°é™åˆ¶ï¼ˆmax_tokens, context_windowï¼‰
2. è‡ªåŠ¨éªŒè¯å’Œè°ƒæ•´å‚æ•°
3. æä¾›å®‰å…¨çš„ API è°ƒç”¨æ–¹æ³•
4. å¤„ç†å‚æ•°é”™è¯¯å¹¶è‡ªåŠ¨é‡è¯•
"""

import logging
from typing import Optional, Dict, Any
import aisuite as ai

logger = logging.getLogger(__name__)


class ModelAdapter:
    """æ¨¡å‹é€‚é…å™¨ - ç»Ÿä¸€ç®¡ç†ä¸åŒæ¨¡å‹çš„å‚æ•°é™åˆ¶"""

    # æ¨¡å‹é™åˆ¶é…ç½®
    MODEL_LIMITS = {
        "deepseek:deepseek-chat": {
            "max_tokens": 8192,
            "context_window": 32768,
            "supports_streaming": True
        },
        "deepseek:deepseek-reasoner": {
            "max_tokens": 8192,
            "context_window": 65536,
            "supports_streaming": False
        },
        "openai:gpt-4o-mini": {
            "max_tokens": 16384,
            "context_window": 128000,
            "supports_streaming": True
        },
        "openai:gpt-4o": {
            "max_tokens": 16384,
            "context_window": 128000,
            "supports_streaming": True
        },
        "openai:o1-mini": {
            "max_tokens": 65536,
            "context_window": 128000,
            "supports_streaming": False
        }
    }

    @classmethod
    def get_model_limits(cls, model: str) -> Dict[str, Any]:
        """
        è·å–æ¨¡å‹é™åˆ¶

        Args:
            model: æ¨¡å‹åç§°ï¼Œå¦‚ "deepseek:deepseek-chat"

        Returns:
            åŒ…å« max_tokens å’Œ context_window çš„å­—å…¸
        """
        limits = cls.MODEL_LIMITS.get(model)
        if limits:
            return limits

        # æœªçŸ¥æ¨¡å‹ï¼Œè¿”å›ä¿å®ˆé»˜è®¤å€¼
        logger.warning(f"æœªçŸ¥æ¨¡å‹ {model}ï¼Œä½¿ç”¨ä¿å®ˆé»˜è®¤é™åˆ¶")
        return {
            "max_tokens": 4096,
            "context_window": 8192,
            "supports_streaming": True
        }

    @classmethod
    def validate_and_adjust_params(cls, model: str, **kwargs) -> Dict[str, Any]:
        """
        éªŒè¯å¹¶è°ƒæ•´å‚æ•°

        Args:
            model: æ¨¡å‹åç§°
            **kwargs: API è°ƒç”¨å‚æ•°

        Returns:
            è°ƒæ•´åçš„å‚æ•°å­—å…¸
        """
        limits = cls.get_model_limits(model)
        adjusted = kwargs.copy()

        # è°ƒæ•´ max_tokens
        if 'max_tokens' in adjusted:
            requested = adjusted['max_tokens']
            max_allowed = limits['max_tokens']

            if requested > max_allowed:
                logger.warning(
                    f"âš ï¸ max_tokens {requested} è¶…è¿‡æ¨¡å‹ {model} çš„é™åˆ¶ {max_allowed}ï¼Œ"
                    f"è‡ªåŠ¨è°ƒæ•´ä¸º {max_allowed}"
                )
                adjusted['max_tokens'] = max_allowed
        else:
            # å¦‚æœæœªè®¾ç½®ï¼Œä½¿ç”¨æ¨¡å‹é™åˆ¶çš„ 80% ä½œä¸ºé»˜è®¤å€¼
            default_max_tokens = int(limits['max_tokens'] * 0.8)
            adjusted['max_tokens'] = default_max_tokens
            logger.debug(f"æœªè®¾ç½® max_tokensï¼Œä½¿ç”¨é»˜è®¤å€¼ {default_max_tokens}")

        return adjusted

    @classmethod
    def safe_api_call(cls, client: ai.Client, model: str, messages: list, **kwargs):
        """
        å®‰å…¨çš„ API è°ƒç”¨ï¼ˆå¸¦å‚æ•°éªŒè¯å’Œé”™è¯¯é‡è¯•ï¼‰

        Args:
            client: aisuite å®¢æˆ·ç«¯å®ä¾‹
            model: æ¨¡å‹åç§°
            messages: æ¶ˆæ¯åˆ—è¡¨
            **kwargs: å…¶ä»– API å‚æ•°

        Returns:
            API å“åº”å¯¹è±¡

        Raises:
            Exception: æ‰€æœ‰é‡è¯•å¤±è´¥åæŠ›å‡ºåŸå§‹å¼‚å¸¸
        """
        import time
        max_retries = 3  # å¢åŠ åˆ° 3 æ¬¡é‡è¯•
        base_wait_time = 2  # åŸºç¡€ç­‰å¾…æ—¶é—´ï¼ˆç§’ï¼‰

        for attempt in range(max_retries):
            try:
                # 1. éªŒè¯å’Œè°ƒæ•´å‚æ•°
                adjusted_params = cls.validate_and_adjust_params(model, **kwargs)

                logger.info(
                    f"ğŸ“¡ è°ƒç”¨ {model} API (å°è¯• {attempt + 1}/{max_retries}), "
                    f"max_tokens={adjusted_params.get('max_tokens')}"
                )

                # 2. è°ƒç”¨ API
                response = client.chat.completions.create(
                    model=model,
                    messages=messages,
                    **adjusted_params
                )

                # 3. æˆåŠŸè¿”å›
                logger.info(f"âœ… {model} API è°ƒç”¨æˆåŠŸ")
                return response

            except (BrokenPipeError, ConnectionError, OSError) as e:
                # è¿æ¥é”™è¯¯ - ä½¿ç”¨æŒ‡æ•°é€€é¿é‡è¯•
                error_name = type(e).__name__
                logger.warning(
                    f"âš ï¸ è¿æ¥é”™è¯¯ ({error_name}): {str(e)[:100]} "
                    f"(å°è¯• {attempt + 1}/{max_retries})"
                )

                if attempt < max_retries - 1:
                    # æŒ‡æ•°é€€é¿ï¼š2s, 4s, 8s
                    wait_time = base_wait_time * (2 ** attempt)
                    logger.info(f"ğŸ”„ ç­‰å¾… {wait_time}s åé‡è¯•...")
                    time.sleep(wait_time)
                    continue
                else:
                    logger.error(f"âŒ è¿æ¥é”™è¯¯ï¼Œæ‰€æœ‰é‡è¯•å·²ç”¨å°½")
                    raise

            except TimeoutError as e:
                # è¶…æ—¶é”™è¯¯ - é‡è¯•
                logger.warning(
                    f"âš ï¸ è¯·æ±‚è¶…æ—¶: {str(e)[:100]} "
                    f"(å°è¯• {attempt + 1}/{max_retries})"
                )

                if attempt < max_retries - 1:
                    wait_time = base_wait_time * (2 ** attempt)
                    logger.info(f"ğŸ”„ ç­‰å¾… {wait_time}s åé‡è¯•...")
                    time.sleep(wait_time)
                    continue
                else:
                    logger.error(f"âŒ è¶…æ—¶é”™è¯¯ï¼Œæ‰€æœ‰é‡è¯•å·²ç”¨å°½")
                    raise

            except Exception as e:
                error_str = str(e)
                error_type = type(e).__name__
                logger.warning(
                    f"âš ï¸ API è°ƒç”¨å¤±è´¥ ({error_type}): {error_str[:200]} "
                    f"(å°è¯• {attempt + 1}/{max_retries})"
                )

                # 4. å‚æ•°é”™è¯¯å¤„ç†
                if "max_tokens" in error_str or "400" in error_str:
                    # å‚æ•°é”™è¯¯ï¼Œè¿›ä¸€æ­¥é™ä½ max_tokens
                    if attempt < max_retries - 1:
                        if 'max_tokens' in adjusted_params:
                            # å‡åŠé‡è¯•
                            old_value = adjusted_params['max_tokens']
                            adjusted_params['max_tokens'] = old_value // 2
                            kwargs['max_tokens'] = adjusted_params['max_tokens']
                            logger.info(
                                f"ğŸ”§ å‚æ•°é”™è¯¯ï¼Œé™ä½ max_tokens: {old_value} â†’ "
                                f"{adjusted_params['max_tokens']}ï¼Œé‡è¯•ä¸­..."
                            )
                            continue

                # 5. é€Ÿç‡é™åˆ¶å¤„ç†
                if "429" in error_str or "rate_limit" in error_str.lower():
                    if attempt < max_retries - 1:
                        wait_time = base_wait_time * (2 ** attempt) * 2  # é€Ÿç‡é™åˆ¶ç­‰å¾…æ›´ä¹…
                        logger.info(f"â³ é€Ÿç‡é™åˆ¶ï¼Œç­‰å¾… {wait_time}s åé‡è¯•...")
                        time.sleep(wait_time)
                        continue

                # 6. å…¶ä»–å¯é‡è¯•é”™è¯¯
                retriable_errors = ["broken pipe", "connection reset", "connection refused"]
                if any(err in error_str.lower() for err in retriable_errors):
                    if attempt < max_retries - 1:
                        wait_time = base_wait_time * (2 ** attempt)
                        logger.info(f"ğŸ”„ ç½‘ç»œé”™è¯¯ï¼Œç­‰å¾… {wait_time}s åé‡è¯•...")
                        time.sleep(wait_time)
                        continue

                # 7. å¦‚æœæ˜¯æœ€åä¸€æ¬¡å°è¯•ï¼Œåˆ™æŠ›å‡ºå¼‚å¸¸
                if attempt == max_retries - 1:
                    logger.error(f"âŒ {model} API è°ƒç”¨å¤±è´¥ï¼Œæ‰€æœ‰é‡è¯•å·²ç”¨å°½")
                    raise

        # ç†è®ºä¸Šä¸ä¼šåˆ°è¿™é‡Œ
        raise RuntimeError("Unexpected error in safe_api_call")

    @classmethod
    def estimate_tokens(cls, text: str) -> int:
        """
        ä¼°ç®—æ–‡æœ¬çš„ token æ•°é‡

        ä½¿ç”¨ç®€å•çš„å¯å‘å¼æ–¹æ³•ï¼š
        - è‹±æ–‡: ~4 å­—ç¬¦ = 1 token
        - ä¸­æ–‡: ~1.5 å­—ç¬¦ = 1 token

        Args:
            text: è¾“å…¥æ–‡æœ¬

        Returns:
            ä¼°ç®—çš„ token æ•°é‡
        """
        if not text:
            return 0

        # ç®€å•çš„å¯å‘å¼ä¼°ç®—
        # ç»Ÿè®¡è‹±æ–‡å’Œä¸­æ–‡å­—ç¬¦
        chinese_chars = sum(1 for char in text if '\u4e00' <= char <= '\u9fff')
        english_chars = len(text) - chinese_chars

        # è‹±æ–‡çº¦ 4 å­—ç¬¦ 1 tokenï¼Œä¸­æ–‡çº¦ 1.5 å­—ç¬¦ 1 token
        estimated_tokens = (english_chars / 4) + (chinese_chars / 1.5)

        return int(estimated_tokens)

    @classmethod
    def get_context_usage(cls, model: str, input_text: str) -> float:
        """
        è®¡ç®—ä¸Šä¸‹æ–‡çª—å£ä½¿ç”¨ç‡

        Args:
            model: æ¨¡å‹åç§°
            input_text: è¾“å…¥æ–‡æœ¬

        Returns:
            ä½¿ç”¨ç‡ (0.0 åˆ° 1.0)
        """
        limits = cls.get_model_limits(model)
        estimated_tokens = cls.estimate_tokens(input_text)
        context_window = limits['context_window']

        usage = estimated_tokens / context_window

        if usage > 0.9:
            logger.warning(
                f"âš ï¸ ä¸Šä¸‹æ–‡ä½¿ç”¨ç‡è¿‡é«˜: {usage:.1%} "
                f"({estimated_tokens}/{context_window} tokens)"
            )

        return usage


# åˆ›å»ºé»˜è®¤å®¢æˆ·ç«¯å®ä¾‹ï¼ˆç”¨äºå‘åå…¼å®¹ï¼‰
_default_client = None


def get_default_client() -> ai.Client:
    """è·å–é»˜è®¤çš„ aisuite å®¢æˆ·ç«¯"""
    global _default_client
    if _default_client is None:
        _default_client = ai.Client()
    return _default_client
