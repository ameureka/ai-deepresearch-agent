"""
åˆ†å—å¤„ç†å™¨ - å¤„ç†è¶…é•¿æ–‡æœ¬çš„åˆ†å—å’Œåˆå¹¶

åŠŸèƒ½:
1. æŒ‰è¯­ä¹‰è¾¹ç•Œï¼ˆæ®µè½ï¼‰æ™ºèƒ½åˆ†å—
2. ä¿æŒå—é—´ä¸Šä¸‹æ–‡è¿è´¯æ€§ï¼ˆé‡å åŒºåŸŸï¼‰
3. æ™ºèƒ½åˆå¹¶å¤„ç†ç»“æœ
4. æ”¯æŒè‡ªå®šä¹‰å—å¤§å°å’Œé‡å 
"""

import logging
import re
from typing import List, Callable, Dict, Any, Optional
from src.model_adapter import ModelAdapter

logger = logging.getLogger(__name__)


class ChunkingProcessor:
    """åˆ†å—å¤„ç†å™¨ - å°†é•¿æ–‡æœ¬åˆ†å—å¤„ç†ååˆå¹¶"""

    def __init__(self, max_chunk_size: int = 6000, overlap_size: int = 200):
        """
        åˆå§‹åŒ–åˆ†å—å¤„ç†å™¨

        Args:
            max_chunk_size: å•ä¸ªå—çš„æœ€å¤§ token æ•°ï¼ˆé»˜è®¤: 6000ï¼‰
            overlap_size: å—é—´é‡å çš„ token æ•°ï¼ˆé»˜è®¤: 200ï¼‰
        """
        self.max_chunk_size = max_chunk_size
        self.overlap_size = overlap_size

    def chunk_by_semantic(self, text: str) -> List[str]:
        """
        æŒ‰è¯­ä¹‰è¾¹ç•Œåˆ†å—ï¼ˆä¿æŒæ®µè½å®Œæ•´ï¼‰

        Args:
            text: éœ€è¦åˆ†å—çš„æ–‡æœ¬

        Returns:
            åˆ†å—åçš„æ–‡æœ¬åˆ—è¡¨
        """
        # 1. æŒ‰æ®µè½åˆ†å‰²ï¼ˆåŒæ¢è¡Œï¼‰
        paragraphs = text.split('\n\n')

        chunks = []
        current_chunk = []
        current_tokens = 0

        for para in paragraphs:
            para_tokens = ModelAdapter.estimate_tokens(para)

            # å¦‚æœåŠ ä¸Šè¿™æ®µä¼šè¶…å‡ºé™åˆ¶
            if current_tokens + para_tokens > self.max_chunk_size:
                # ä¿å­˜å½“å‰å—
                if current_chunk:
                    chunks.append('\n\n'.join(current_chunk))
                    current_chunk = []
                    current_tokens = 0

            # å¦‚æœå•æ®µå°±è¶…äº†ï¼Œå¼ºåˆ¶åˆ†å‰²
            if para_tokens > self.max_chunk_size:
                sub_chunks = self._split_long_paragraph(para)
                # å°†å­å—æ·»åŠ åˆ°ç»“æœ
                for sub_chunk in sub_chunks:
                    chunks.append(sub_chunk)
            else:
                current_chunk.append(para)
                current_tokens += para_tokens

        # æœ€åä¸€å—
        if current_chunk:
            chunks.append('\n\n'.join(current_chunk))

        logger.info(f"ğŸ“¦ æ–‡æœ¬åˆ†å—å®Œæˆ: {len(chunks)} å—")
        return chunks

    def _split_long_paragraph(self, para: str) -> List[str]:
        """
        å¼ºåˆ¶åˆ†å‰²è¶…é•¿æ®µè½ï¼ˆæŒ‰å¥å­åˆ†å‰²ï¼‰

        Args:
            para: è¶…é•¿æ®µè½

        Returns:
            åˆ†å‰²åçš„å­å—åˆ—è¡¨
        """
        # æŒ‰å¥å­åˆ†å‰²ï¼ˆæ”¯æŒä¸­è‹±æ–‡ï¼‰
        # è‹±æ–‡: . ! ? åè·Ÿç©ºæ ¼
        # ä¸­æ–‡: ã€‚ï¼ï¼Ÿ
        sentences = re.split(r'([.!?ã€‚ï¼ï¼Ÿ])\s+', para)

        # é‡æ–°ç»„åˆå¥å­å’Œæ ‡ç‚¹
        full_sentences = []
        for i in range(0, len(sentences) - 1, 2):
            if i + 1 < len(sentences):
                full_sentences.append(sentences[i] + sentences[i + 1])
            else:
                full_sentences.append(sentences[i])

        # å¦‚æœæ²¡æœ‰æˆåŠŸåˆ†å‰²ï¼ŒæŒ‰å›ºå®šé•¿åº¦å¼ºåˆ¶åˆ†å‰²
        if len(full_sentences) <= 1:
            # æŒ‰å­—ç¬¦æ•°å¼ºåˆ¶åˆ†å‰²ï¼ˆä¼°ç®— 4 å­—ç¬¦ = 1 tokenï¼‰
            chunk_chars = self.max_chunk_size * 4
            return [para[i:i+chunk_chars] for i in range(0, len(para), chunk_chars)]

        # æŒ‰å¥å­ç»„åˆæˆå—
        chunks = []
        current_chunk = []
        current_tokens = 0

        for sent in full_sentences:
            sent_tokens = ModelAdapter.estimate_tokens(sent)

            if current_tokens + sent_tokens > self.max_chunk_size:
                if current_chunk:
                    chunks.append(' '.join(current_chunk))
                    current_chunk = []
                    current_tokens = 0

            current_chunk.append(sent)
            current_tokens += sent_tokens

        if current_chunk:
            chunks.append(' '.join(current_chunk))

        return chunks

    def process_with_context(
        self,
        chunks: List[str],
        processor_func: Callable[[str], str],
        show_progress: bool = True
    ) -> List[str]:
        """
        å¸¦ä¸Šä¸‹æ–‡å¤„ç†æ¯ä¸ªå—

        Args:
            chunks: æ–‡æœ¬å—åˆ—è¡¨
            processor_func: å¤„ç†å‡½æ•°ï¼Œæ¥æ”¶æ–‡æœ¬è¿”å›å¤„ç†ç»“æœ
            show_progress: æ˜¯å¦æ˜¾ç¤ºè¿›åº¦ï¼ˆé»˜è®¤: Trueï¼‰

        Returns:
            å¤„ç†ç»“æœåˆ—è¡¨
        """
        results = []

        for i, chunk in enumerate(chunks):
            if show_progress:
                logger.info(f"ğŸ“ å¤„ç†å— {i+1}/{len(chunks)}...")

            # æ„å»ºä¸Šä¸‹æ–‡ä¿¡æ¯
            context_info = {
                'position': f"{i+1}/{len(chunks)}",
                'is_first': i == 0,
                'is_last': i == len(chunks) - 1,
                'prev_text': self._get_overlap(chunks[i-1], is_end=True) if i > 0 else None,
                'next_text': self._get_overlap(chunks[i+1], is_end=False) if i < len(chunks)-1 else None
            }

            # æ„å»ºå¸¦ä¸Šä¸‹æ–‡çš„æç¤º
            prompt = self._build_chunk_prompt(chunk, context_info)

            # å¤„ç†
            result = processor_func(prompt)
            results.append(result)

        return results

    def _get_overlap(self, text: str, is_end: bool) -> str:
        """
        è·å–é‡å åŒºåŸŸçš„æ–‡æœ¬

        Args:
            text: æºæ–‡æœ¬
            is_end: True è¡¨ç¤ºå–ç»“å°¾ï¼ŒFalse è¡¨ç¤ºå–å¼€å¤´

        Returns:
            é‡å åŒºåŸŸçš„æ–‡æœ¬
        """
        # ä¼°ç®—é‡å å­—ç¬¦æ•°ï¼ˆæŒ‰ 4 å­—ç¬¦ = 1 tokenï¼‰
        overlap_chars = self.overlap_size * 4

        if is_end:
            # å–æœ€å overlap_size tokens
            return text[-overlap_chars:] if len(text) > overlap_chars else text
        else:
            # å–å‰ overlap_size tokens
            return text[:overlap_chars] if len(text) > overlap_chars else text

    def _build_chunk_prompt(self, chunk: str, context: Dict[str, Any]) -> str:
        """
        æ„å»ºå¸¦ä¸Šä¸‹æ–‡çš„æç¤º

        Args:
            chunk: å½“å‰æ–‡æœ¬å—
            context: ä¸Šä¸‹æ–‡ä¿¡æ¯å­—å…¸

        Returns:
            å®Œæ•´çš„æç¤ºæ–‡æœ¬
        """
        parts = []

        # æ·»åŠ ä½ç½®ä¿¡æ¯
        parts.append(f"[ğŸ“ æ–‡æ¡£ä½ç½®: {context['position']}]")

        # æ·»åŠ å‰æ–‡
        if context['prev_text']:
            parts.append(f"[â¬†ï¸ å‰æ–‡ç»“å°¾]:\n...{context['prev_text']}\n")

        # æ·»åŠ å½“å‰å†…å®¹
        parts.append(f"[ğŸ“„ å½“å‰æ®µè½]:\n{chunk}\n")

        # æ·»åŠ åæ–‡
        if context['next_text']:
            parts.append(f"[â¬‡ï¸ åæ–‡å¼€å¤´]:\n{context['next_text']}...\n")

        # æ·»åŠ å¤„ç†è¯´æ˜
        if context['is_first']:
            parts.append("â„¹ï¸ è¿™æ˜¯æ–‡æ¡£çš„ç¬¬ä¸€éƒ¨åˆ†ã€‚")
        elif context['is_last']:
            parts.append("â„¹ï¸ è¿™æ˜¯æ–‡æ¡£çš„æœ€åä¸€éƒ¨åˆ†ã€‚")
        else:
            parts.append("â„¹ï¸ è¿™æ˜¯æ–‡æ¡£çš„ä¸­é—´éƒ¨åˆ†ï¼Œè¯·æ³¨æ„ä¸å‰åæ–‡çš„è¿è´¯æ€§ã€‚")

        parts.append("\nè¯·å¤„ç†å½“å‰æ®µè½ï¼Œç¡®ä¿ä¸å‰åæ–‡ä¿æŒè¿è´¯ã€‚")

        return '\n'.join(parts)

    def merge_chunks(self, chunks: List[str], remove_redundancy: bool = True) -> str:
        """
        åˆå¹¶å¤„ç†åçš„å—

        Args:
            chunks: å¤„ç†åçš„æ–‡æœ¬å—åˆ—è¡¨
            remove_redundancy: æ˜¯å¦ç§»é™¤é‡å åŒºåŸŸçš„å†—ä½™å†…å®¹ï¼ˆé»˜è®¤: Trueï¼‰

        Returns:
            åˆå¹¶åçš„æ–‡æœ¬
        """
        if not chunks:
            return ""

        if len(chunks) == 1:
            return chunks[0]

        # ç®€å•åˆå¹¶ï¼ˆç”¨åŒæ¢è¡Œè¿æ¥ï¼‰
        # TODO: æœªæ¥å¯ä»¥å®ç°æ›´æ™ºèƒ½çš„åˆå¹¶ï¼Œæ£€æµ‹å’Œç§»é™¤é‡å¤å†…å®¹
        merged = '\n\n'.join(chunks)

        logger.info(f"âœ… åˆå¹¶å®Œæˆ: {len(chunks)} å— â†’ 1 ä¸ªæ–‡æ¡£")
        return merged

    def chunk_and_process(
        self,
        text: str,
        processor_func: Callable[[str], str],
        show_progress: bool = True
    ) -> str:
        """
        ä¸€ç«™å¼åˆ†å—å¤„ç†ï¼šåˆ†å— â†’ å¤„ç† â†’ åˆå¹¶

        Args:
            text: éœ€è¦å¤„ç†çš„æ–‡æœ¬
            processor_func: å¤„ç†å‡½æ•°
            show_progress: æ˜¯å¦æ˜¾ç¤ºè¿›åº¦

        Returns:
            æœ€ç»ˆå¤„ç†ç»“æœ
        """
        logger.info(f"ğŸš€ å¼€å§‹åˆ†å—å¤„ç† (ä¼°ç®—: {ModelAdapter.estimate_tokens(text)} tokens)")

        # 1. åˆ†å—
        chunks = self.chunk_by_semantic(text)

        # 2. å¤„ç†
        results = self.process_with_context(chunks, processor_func, show_progress)

        # 3. åˆå¹¶
        final_result = self.merge_chunks(results)

        logger.info("ğŸ‰ åˆ†å—å¤„ç†å®Œæˆ")
        return final_result
