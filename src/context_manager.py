"""
ä¸Šä¸‹æ–‡ç®¡ç†å™¨ - æ™ºèƒ½å†³ç­–æ–‡æœ¬å¤„ç†ç­–ç•¥

åŠŸèƒ½:
1. ç›‘æ§ä¸Šä¸‹æ–‡é•¿åº¦
2. å†³å®šå¤„ç†ç­–ç•¥ï¼ˆç›´æ¥/åˆ†å—ï¼‰
3. ç®¡ç†æ–‡æœ¬å¤„ç†æµç¨‹
4. æ”¯æŒé…ç½®åŒ–çš„é˜ˆå€¼
"""

import logging
import os
from typing import Callable, Optional
from src.model_adapter import ModelAdapter
from src.chunking import ChunkingProcessor

logger = logging.getLogger(__name__)


class ContextManager:
    """ä¸Šä¸‹æ–‡ç®¡ç†å™¨ - æ™ºèƒ½é€‰æ‹©æ–‡æœ¬å¤„ç†ç­–ç•¥"""

    def __init__(
        self,
        model: str,
        enable_chunking: bool = None,
        chunking_threshold: float = None,
        max_chunk_size: int = None,
        chunk_overlap: int = None
    ):
        """
        åˆå§‹åŒ–ä¸Šä¸‹æ–‡ç®¡ç†å™¨

        Args:
            model: æ¨¡å‹åç§°
            enable_chunking: æ˜¯å¦å¯ç”¨åˆ†å—ï¼ˆNone è¡¨ç¤ºä»ç¯å¢ƒå˜é‡è¯»å–ï¼‰
            chunking_threshold: åˆ†å—é˜ˆå€¼ï¼ˆä¸Šä¸‹æ–‡çª—å£çš„ç™¾åˆ†æ¯”ï¼ŒNone è¡¨ç¤ºä»ç¯å¢ƒå˜é‡è¯»å–ï¼‰
            max_chunk_size: æœ€å¤§å—å¤§å°ï¼ˆNone è¡¨ç¤ºä»ç¯å¢ƒå˜é‡è¯»å–ï¼‰
            chunk_overlap: å—é‡å å¤§å°ï¼ˆNone è¡¨ç¤ºä»ç¯å¢ƒå˜é‡è¯»å–ï¼‰
        """
        self.model = model
        self.limits = ModelAdapter.get_model_limits(model)

        # ä»ç¯å¢ƒå˜é‡æˆ–å‚æ•°è·å–é…ç½®
        self.enable_chunking = (
            enable_chunking
            if enable_chunking is not None
            else os.getenv('ENABLE_CHUNKING', 'true').lower() == 'true'
        )

        self.chunking_threshold = (
            chunking_threshold
            if chunking_threshold is not None
            else float(os.getenv('CHUNKING_THRESHOLD', '0.8'))
        )

        max_chunk_size = (
            max_chunk_size
            if max_chunk_size is not None
            else int(os.getenv('MAX_CHUNK_SIZE', '6000'))
        )

        chunk_overlap = (
            chunk_overlap
            if chunk_overlap is not None
            else int(os.getenv('CHUNK_OVERLAP', '200'))
        )

        # åˆ›å»ºåˆ†å—å¤„ç†å™¨
        self.chunking_processor = ChunkingProcessor(
            max_chunk_size=max_chunk_size,
            overlap_size=chunk_overlap
        )

        logger.info(
            f"ğŸ“Š ä¸Šä¸‹æ–‡ç®¡ç†å™¨åˆå§‹åŒ–: æ¨¡å‹={model}, "
            f"å¯ç”¨åˆ†å—={self.enable_chunking}, "
            f"é˜ˆå€¼={self.chunking_threshold:.0%}"
        )

    def should_chunk(self, text: str) -> bool:
        """
        åˆ¤æ–­æ˜¯å¦éœ€è¦åˆ†å—å¤„ç†

        Args:
            text: è¾“å…¥æ–‡æœ¬

        Returns:
            True è¡¨ç¤ºéœ€è¦åˆ†å—ï¼ŒFalse è¡¨ç¤ºå¯ä»¥ç›´æ¥å¤„ç†
        """
        if not self.enable_chunking:
            return False

        estimated_tokens = ModelAdapter.estimate_tokens(text)
        threshold_tokens = int(self.limits['context_window'] * self.chunking_threshold)

        needs_chunking = estimated_tokens > threshold_tokens

        if needs_chunking:
            logger.info(
                f"âš ï¸ æ–‡æœ¬è¶…è¿‡é˜ˆå€¼: {estimated_tokens} tokens > "
                f"{threshold_tokens} tokens ({self.chunking_threshold:.0%} of {self.limits['context_window']}), "
                f"å°†å¯ç”¨åˆ†å—å¤„ç†"
            )
        else:
            logger.debug(
                f"âœ… æ–‡æœ¬åœ¨é˜ˆå€¼å†…: {estimated_tokens} tokens <= "
                f"{threshold_tokens} tokens, ç›´æ¥å¤„ç†"
            )

        return needs_chunking

    def get_context_usage(self, text: str) -> float:
        """
        è®¡ç®—ä¸Šä¸‹æ–‡ä½¿ç”¨ç‡

        Args:
            text: è¾“å…¥æ–‡æœ¬

        Returns:
            ä½¿ç”¨ç‡ (0.0 åˆ° 1.0+)
        """
        return ModelAdapter.get_context_usage(self.model, text)

    def process_text(
        self,
        text: str,
        processor_func: Callable[[str], str],
        force_chunking: bool = False,
        show_progress: bool = True
    ) -> str:
        """
        æ™ºèƒ½å¤„ç†æ–‡æœ¬ï¼ˆè‡ªåŠ¨é€‰æ‹©ç­–ç•¥ï¼‰

        Args:
            text: éœ€è¦å¤„ç†çš„æ–‡æœ¬
            processor_func: å¤„ç†å‡½æ•°ï¼Œæ¥æ”¶æ–‡æœ¬è¿”å›å¤„ç†ç»“æœ
            force_chunking: å¼ºåˆ¶ä½¿ç”¨åˆ†å—ï¼ˆé»˜è®¤: Falseï¼‰
            show_progress: æ˜¯å¦æ˜¾ç¤ºè¿›åº¦ï¼ˆé»˜è®¤: Trueï¼‰

        Returns:
            å¤„ç†åçš„æ–‡æœ¬
        """
        # 1. è¯„ä¼°æ˜¯å¦éœ€è¦åˆ†å—
        needs_chunking = force_chunking or self.should_chunk(text)

        # 2. æ ¹æ®ç­–ç•¥å¤„ç†
        if not needs_chunking:
            # ç›´æ¥å¤„ç†
            logger.info("ğŸ“ ç›´æ¥å¤„ç†æ¨¡å¼")
            return processor_func(text)
        else:
            # åˆ†å—å¤„ç†
            logger.info("ğŸ“¦ åˆ†å—å¤„ç†æ¨¡å¼")
            return self.chunking_processor.chunk_and_process(
                text=text,
                processor_func=processor_func,
                show_progress=show_progress
            )

    def estimate_cost(self, text: str, cost_per_1k_tokens: float = 0.14) -> dict:
        """
        ä¼°ç®—å¤„ç†æˆæœ¬

        Args:
            text: è¾“å…¥æ–‡æœ¬
            cost_per_1k_tokens: æ¯ 1K tokens çš„æˆæœ¬ï¼ˆé»˜è®¤: DeepSeek chat è¾“å…¥ä»·æ ¼ï¼‰

        Returns:
            åŒ…å«ä¼°ç®—ä¿¡æ¯çš„å­—å…¸
        """
        estimated_tokens = ModelAdapter.estimate_tokens(text)
        needs_chunking = self.should_chunk(text)

        if not needs_chunking:
            # ç›´æ¥å¤„ç†ï¼š1 æ¬¡è°ƒç”¨
            calls = 1
            total_tokens = estimated_tokens
        else:
            # åˆ†å—å¤„ç†ï¼šéœ€è¦å¤šæ¬¡è°ƒç”¨
            num_chunks = (estimated_tokens // self.chunking_processor.max_chunk_size) + 1
            calls = num_chunks
            # è€ƒè™‘é‡å åŒºåŸŸä¼šå¢åŠ æ€» token æ•°
            overlap_overhead = (num_chunks - 1) * self.chunking_processor.overlap_size
            total_tokens = estimated_tokens + overlap_overhead

        estimated_cost = (total_tokens / 1000) * cost_per_1k_tokens

        return {
            'input_tokens': estimated_tokens,
            'total_tokens_with_overhead': total_tokens,
            'needs_chunking': needs_chunking,
            'num_chunks': calls if needs_chunking else 1,
            'api_calls': calls,
            'estimated_cost_usd': estimated_cost,
            'context_usage': self.get_context_usage(text)
        }


# ä¾¿æ·å‡½æ•°ï¼šåˆ›å»ºé€‚ç”¨äºç‰¹å®š Agent çš„ä¸Šä¸‹æ–‡ç®¡ç†å™¨
def create_manager_for_agent(agent_name: str, model: str) -> ContextManager:
    """
    ä¸ºç‰¹å®š Agent åˆ›å»ºä¸Šä¸‹æ–‡ç®¡ç†å™¨

    Args:
        agent_name: Agent åç§°ï¼ˆç”¨äºæ—¥å¿—ï¼‰
        model: æ¨¡å‹åç§°

    Returns:
        é…ç½®å¥½çš„ ContextManager å®ä¾‹
    """
    logger.info(f"ğŸ”§ ä¸º {agent_name} åˆ›å»ºä¸Šä¸‹æ–‡ç®¡ç†å™¨")
    return ContextManager(model=model)
