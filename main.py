"""
ä¸»åº”ç”¨ç¨‹åº - å¤šä»£ç†ç ”ç©¶æŠ¥å‘Šç”Ÿæˆç³»ç»Ÿçš„ Web æœåŠ¡
æœ¬æ–‡ä»¶å®ç°äº†ä¸€ä¸ª FastAPI Web åº”ç”¨ï¼Œæä¾›ï¼š
1. ç”¨æˆ·ç•Œé¢ç”¨äºæäº¤ç ”ç©¶ä¸»é¢˜
2. åå°ä»»åŠ¡å¤„ç†ç³»ç»Ÿ
3. å®æ—¶è¿›åº¦è·Ÿè¸ª
4. æ•°æ®åº“æŒä¹…åŒ–
5. Phase 2: æ ‡å‡†åŒ– API å’Œ SSE æµå¼æ¥å£
"""

from __future__ import annotations

import os
import uuid
import json
import threading
import logging
import traceback
from datetime import datetime
from queue import Queue
from typing import Optional, Literal, Dict, Any
from fastapi import FastAPI, HTTPException, Request, status
from fastapi.responses import HTMLResponse, JSONResponse
from fastapi.staticfiles import StaticFiles
from fastapi.templating import Jinja2Templates
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, ValidationError
from sqlalchemy import create_engine, Column, Text, DateTime, String
from sqlalchemy.orm import sessionmaker, declarative_base
from sqlalchemy.dialects.postgresql import UUID as PGUUID, JSONB
from dotenv import load_dotenv

from src.planning_agent import planner_agent, executor_agent_step
from src.api_models import ApiResponse, ResearchRequest, HealthResponse, ModelInfo
from src.sse import (
    format_sse_event,
    SSEEvents,
    get_sse_headers,
    create_start_event,
    create_plan_event,
    create_progress_event,
    create_done_event,
    create_error_event
)
from fastapi.responses import StreamingResponse

import html, textwrap

# === é…ç½®ç»“æ„åŒ–æ—¥å¿— ===
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)

# === åŠ è½½ç¯å¢ƒå˜é‡ ===
load_dotenv()
DATABASE_URL = os.getenv("DATABASE_URL")

# ä¿®å¤ Heroku çš„ postgres:// URL æ ¼å¼ï¼ˆéœ€è¦ä½¿ç”¨ postgresql://ï¼‰
if DATABASE_URL.startswith("postgres://"):
    DATABASE_URL = DATABASE_URL.replace("postgres://", "postgresql://", 1)

if not DATABASE_URL:
    raise RuntimeError("DATABASE_URL æœªè®¾ç½®")


# === æ•°æ®åº“è®¾ç½® ===
Base = declarative_base()
engine = create_engine(DATABASE_URL, echo=False, future=True)
SessionLocal = sessionmaker(bind=engine)

research_task_queue: Queue = Queue()
worker_thread: Optional[threading.Thread] = None
worker_stop_event = threading.Event()


def default_progress() -> Dict[str, Any]:
    return {
        "currentStep": None,
        "totalSteps": None,
        "completedSteps": 0,
        "events": [],
    }


def ensure_progress(task: ResearchTask) -> Dict[str, Any]:
    progress = task.progress or {}
    if not isinstance(progress, dict):
        progress = {}
    events = progress.get("events", [])
    if not isinstance(events, list):
        events = list(events) if events else []
    progress["events"] = events
    progress.setdefault("currentStep", None)
    progress.setdefault("totalSteps", None)
    progress.setdefault("completedSteps", 0)
    task.progress = progress
    return progress


def ensure_queue_info(task: "ResearchTask") -> Dict[str, Any]:
    queue_info = task.queue_info or {}
    if not isinstance(queue_info, dict):
        queue_info = {}
    task.queue_info = queue_info
    return queue_info


def add_event(task: ResearchTask, event_type: str, message: str, extra: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
    progress = ensure_progress(task)
    event = {
        "type": event_type,
        "message": message,
        "timestamp": datetime.utcnow().isoformat() + "Z",
    }
    if extra:
        event.update(extra)
    progress["events"].append(event)
    task.progress = progress
    task.updated_at = datetime.utcnow()
    return event


def serialize_progress(progress: Optional[Dict[str, Any]]) -> Dict[str, Any]:
    if not progress or not isinstance(progress, dict):
        return default_progress()
    events = progress.get("events", [])
    if not isinstance(events, list):
        events = list(events)
    return {
        "currentStep": progress.get("currentStep"),
        "totalSteps": progress.get("totalSteps"),
        "completedSteps": progress.get("completedSteps", 0),
        "events": events,
    }


def serialize_queue_info(queue_info: Optional[Dict[str, Any]]) -> Dict[str, Any]:
    if not queue_info or not isinstance(queue_info, dict):
        return {}
    sanitized = {}
    for key, value in queue_info.items():
        if isinstance(value, (str, int, float, type(None))):
            sanitized[key] = value
        else:
            try:
                sanitized[key] = json.loads(json.dumps(value))
            except Exception:
                sanitized[key] = str(value)
    return sanitized


def run_research_task(queue_item: Dict[str, Any]) -> None:
    task_id = queue_item.get("task_id")
    prompt = queue_item.get("prompt")
    model = queue_item.get("model")

    if not task_id:
        logger.error("Queue item missing task_id, skipping execution")
        return

    session = SessionLocal()
    try:
        task: Optional[ResearchTask] = (
            session.query(ResearchTask)
            .filter(ResearchTask.task_id == task_id)
            .one_or_none()
        )

        if not task:
            logger.error(f"ResearchTask with task_id {task_id} not found, skipping execution.")
            return

        if prompt:
            task.topic = prompt
        prompt_to_use = task.topic

        if not prompt_to_use:
            logger.error(f"ResearchTask {task_id} lacks prompt/topic, marking as failed.")
            task.status = "failed"
            add_event(task, "error", "Prompt is required but missing.")
            session.commit()
            return

        queue_info = ensure_queue_info(task)
        if "enqueuedAt" not in queue_info:
            queue_info["enqueuedAt"] = datetime.utcnow().isoformat() + "Z"
        queue_info["startedAt"] = datetime.utcnow().isoformat() + "Z"
        queue_info["workerId"] = threading.current_thread().name
        retry_count = queue_info.get("retryCount", 0)
        if not isinstance(retry_count, int):
            try:
                retry_count = int(retry_count)  # type: ignore[arg-type]
            except Exception:
                retry_count = 0
        queue_info["retryCount"] = retry_count
        task.queue_info = queue_info

        task.status = "running"
        ensure_progress(task)
        add_event(task, "start", "Research started", {"prompt": prompt_to_use})
        task.started_at = datetime.utcnow()
        session.commit()

        execution_history = []

        steps = planner_agent(prompt_to_use, model=model)
        add_event(task, "plan", "Research plan generated", {"steps": steps})
        progress = ensure_progress(task)
        progress["totalSteps"] = len(steps)
        progress["completedSteps"] = 0
        progress["currentStep"] = None
        task.progress = progress
        session.commit()

        for index, step_title in enumerate(steps):
            step_number = index + 1
            add_event(
                task,
                "progress",
                step_title,
                {"step": step_number, "total": len(steps)},
            )
            progress = ensure_progress(task)
            progress["completedSteps"] = step_number
            progress["currentStep"] = step_title
            task.progress = progress
            session.commit()

            step_desc, agent_name, output = executor_agent_step(
                step_title, execution_history, prompt_to_use
            )
            execution_history.append([step_title, step_desc, output])
            logger.info(
                f"Task {task_id}: step {step_number}/{len(steps)} completed using {agent_name}"
            )

        final_report = (
            execution_history[-1][2] if execution_history else "æœªç”ŸæˆæŠ¥å‘Šã€‚"
        )

        add_event(task, "done", "Research completed", {"report": final_report})
        progress = ensure_progress(task)
        progress["completedSteps"] = progress.get("totalSteps") or len(steps)
        progress["currentStep"] = None
        task.progress = progress
        task.status = "completed"
        task.report = final_report
        queue_info = ensure_queue_info(task)
        queue_info["finishedAt"] = datetime.utcnow().isoformat() + "Z"
        task.queue_info = queue_info
        task.completed_at = datetime.utcnow()
        task.updated_at = datetime.utcnow()
        session.commit()
        logger.info(f"Task {task_id} completed successfully.")

    except Exception as exc:
        logger.error(f"Task {task_id} failed: {exc}")
        logger.error(traceback.format_exc())
        session.rollback()
        task = (
            session.query(ResearchTask)
            .filter(ResearchTask.task_id == task_id)
            .one_or_none()
        )
        if task:
            ensure_progress(task)
            add_event(task, "error", f"Task failed: {exc}")
            progress = ensure_progress(task)
            progress["currentStep"] = None
            task.progress = progress
            task.status = "failed"
            queue_info = ensure_queue_info(task)
            queue_info["failedAt"] = datetime.utcnow().isoformat() + "Z"
            task.queue_info = queue_info
            task.failed_at = datetime.utcnow()
            task.updated_at = datetime.utcnow()
            session.commit()
    finally:
        session.close()


def worker_loop() -> None:
    logger.info("Research worker started.")
    while not worker_stop_event.is_set():
        try:
            queue_item = research_task_queue.get()
            if queue_item is None:
                research_task_queue.task_done()
                break
            run_research_task(queue_item)
        except Exception as exc:
            logger.error(f"Unexpected error in worker loop: {exc}")
            logger.error(traceback.format_exc())
        finally:
            research_task_queue.task_done()
    logger.info("Research worker stopped.")


def start_worker() -> None:
    global worker_thread
    if worker_thread and worker_thread.is_alive():
        return
    worker_stop_event.clear()
    worker_thread = threading.Thread(
        target=worker_loop, name="ResearchWorker", daemon=True
    )
    worker_thread.start()


def stop_worker() -> None:
    global worker_thread
    if worker_thread and worker_thread.is_alive():
        worker_stop_event.set()
        research_task_queue.put(None)
        worker_thread.join(timeout=5)
        logger.info("Research worker thread joined.")
        worker_thread = None



class Task(Base):
    """
    ä»»åŠ¡æ•°æ®æ¨¡å‹ - å­˜å‚¨ç ”ç©¶æŠ¥å‘Šç”Ÿæˆä»»åŠ¡çš„ä¿¡æ¯
    """

    __tablename__ = "tasks"
    id = Column(String, primary_key=True, index=True)  # ä»»åŠ¡å”¯ä¸€æ ‡è¯†ç¬¦
    prompt = Column(Text)  # ç”¨æˆ·è¾“å…¥çš„ç ”ç©¶ä¸»é¢˜
    status = Column(String)  # ä»»åŠ¡çŠ¶æ€ï¼šrunning, done, error
    created_at = Column(DateTime, default=datetime.utcnow)  # åˆ›å»ºæ—¶é—´
    updated_at = Column(DateTime, default=datetime.utcnow)  # æ›´æ–°æ—¶é—´
    result = Column(Text)  # ä»»åŠ¡ç»“æœï¼ˆJSON æ ¼å¼ï¼‰


class ResearchTask(Base):
    """
    research_tasks è¡¨æ¨¡å‹ - ä¸ Next.js Drizzle è¡¨ç»“æ„ä¿æŒå¯¹é½
    """

    __tablename__ = "research_tasks"
    __table_args__ = {"extend_existing": True}

    id = Column(PGUUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    task_id = Column(String, unique=True, index=True, nullable=False)
    user_id = Column(PGUUID(as_uuid=True), nullable=True)
    chat_id = Column(PGUUID(as_uuid=True), nullable=True)
    topic = Column(Text, nullable=True)
    status = Column(String, nullable=False, default="queued")
    progress = Column(JSONB, nullable=True)
    report = Column(Text, nullable=True)
    queue_info = Column(JSONB, nullable=True)
    started_at = Column(DateTime, nullable=True)
    completed_at = Column(DateTime, nullable=True)
    failed_at = Column(DateTime, nullable=True)
    created_at = Column(DateTime, default=datetime.utcnow)
    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)


# åˆ›å»ºæ•°æ®åº“è¡¨ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
try:
    Base.metadata.create_all(bind=engine)
    logger.info("âœ… æ•°æ®åº“è¡¨åˆå§‹åŒ–å®Œæˆ")
except Exception as e:
    logger.error(f"âŒ æ•°æ®åº“åˆ›å»ºå¤±è´¥: {e}")
    raise RuntimeError(f"æ•°æ®åº“åˆå§‹åŒ–å¤±è´¥: {e}")

# === FastAPI åº”ç”¨è®¾ç½® ===
app = FastAPI(
    title="AI Research Assistant API",
    description="å¤šä»£ç†ç ”ç©¶æŠ¥å‘Šç”Ÿæˆç³»ç»Ÿ - Phase 2 æ ‡å‡†åŒ– API",
    version="2.0.0"
)


@app.on_event("startup")
async def startup_event():
    start_worker()


@app.on_event("shutdown")
async def shutdown_event():
    stop_worker()

# === Phase 2: é…ç½® CORS ä¸­é—´ä»¶ï¼ˆæ›´ä¸¥æ ¼çš„é…ç½®ï¼‰===
# ä»ç¯å¢ƒå˜é‡è¯»å–å…è®¸çš„æ¥æºï¼Œå¦‚æœæœªè®¾ç½®åˆ™ä½¿ç”¨é»˜è®¤å€¼
ALLOWED_ORIGINS = os.getenv(
    "ALLOWED_ORIGINS",
    "http://localhost:3000,https://*.vercel.app"
).split(",")

# å¤„ç†é€šé…ç¬¦åŸŸåï¼ˆ*.vercel.appï¼‰
# FastAPI çš„ CORSMiddleware æ”¯æŒæ­£åˆ™è¡¨è¾¾å¼
app.add_middleware(
    CORSMiddleware,
    allow_origins=ALLOWED_ORIGINS,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
    expose_headers=["*"]
)

logger.info(f"âœ… CORS é…ç½®å®Œæˆï¼Œå…è®¸çš„æ¥æº: {ALLOWED_ORIGINS}")

# === Phase 2: å…¨å±€å¼‚å¸¸å¤„ç†å™¨ ===
@app.exception_handler(Exception)
async def global_exception_handler(request: Request, exc: Exception):
    """
    å…¨å±€å¼‚å¸¸å¤„ç†å™¨ - æ•è·æ‰€æœ‰æœªå¤„ç†çš„å¼‚å¸¸

    è¿™ç¡®ä¿ï¼š
    1. æ‰€æœ‰é”™è¯¯éƒ½è¿”å›ç»Ÿä¸€çš„ ApiResponse æ ¼å¼
    2. ä¸å‘å®¢æˆ·ç«¯æ³„éœ²æ•æ„Ÿä¿¡æ¯ï¼ˆå¦‚å †æ ˆè·Ÿè¸ªï¼‰
    3. é”™è¯¯è¢«æ­£ç¡®è®°å½•åˆ°æ—¥å¿—ç³»ç»Ÿ

    å‚æ•°ï¼š
        request: FastAPI è¯·æ±‚å¯¹è±¡
        exc: æ•è·çš„å¼‚å¸¸

    è¿”å›ï¼š
        JSONResponse with ApiResponse format
    """
    # è®°å½•è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯åˆ°æ—¥å¿—ï¼ˆåŒ…å«å †æ ˆè·Ÿè¸ªï¼‰
    logger.error(
        f"âŒ æœªå¤„ç†çš„å¼‚å¸¸: {type(exc).__name__}: {str(exc)}\n"
        f"è·¯å¾„: {request.url.path}\n"
        f"æ–¹æ³•: {request.method}\n"
        f"å †æ ˆè·Ÿè¸ª:\n{traceback.format_exc()}"
    )

    # å‘å®¢æˆ·ç«¯è¿”å›ç®€åŒ–çš„é”™è¯¯ä¿¡æ¯ï¼ˆä¸åŒ…å«æ•æ„Ÿä¿¡æ¯ï¼‰
    return JSONResponse(
        status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
        content=ApiResponse(
            success=False,
            error=f"Internal server error: {type(exc).__name__}"
        ).dict()
    )


@app.exception_handler(HTTPException)
async def http_exception_handler(request: Request, exc: HTTPException):
    """
    HTTP å¼‚å¸¸å¤„ç†å™¨ - å¤„ç†æ ‡å‡†çš„ HTTP å¼‚å¸¸

    å°† FastAPI çš„ HTTPException è½¬æ¢ä¸ºæ ‡å‡†çš„ ApiResponse æ ¼å¼ã€‚

    å‚æ•°ï¼š
        request: FastAPI è¯·æ±‚å¯¹è±¡
        exc: HTTPException å®ä¾‹

    è¿”å›ï¼š
        JSONResponse with ApiResponse format
    """
    logger.warning(
        f"âš ï¸ HTTP å¼‚å¸¸ {exc.status_code}: {exc.detail}\n"
        f"è·¯å¾„: {request.url.path}\n"
        f"æ–¹æ³•: {request.method}"
    )

    return JSONResponse(
        status_code=exc.status_code,
        content=ApiResponse(
            success=False,
            error=exc.detail
        ).dict()
    )


@app.exception_handler(ValidationError)
async def validation_exception_handler(request: Request, exc: ValidationError):
    """
    Pydantic éªŒè¯å¼‚å¸¸å¤„ç†å™¨

    å¤„ç†è¯·æ±‚ä½“éªŒè¯å¤±è´¥çš„æƒ…å†µã€‚

    å‚æ•°ï¼š
        request: FastAPI è¯·æ±‚å¯¹è±¡
        exc: ValidationError å®ä¾‹

    è¿”å›ï¼š
        JSONResponse with ApiResponse format
    """
    logger.warning(
        f"âš ï¸ è¯·æ±‚éªŒè¯å¤±è´¥:\n"
        f"è·¯å¾„: {request.url.path}\n"
        f"é”™è¯¯: {exc.errors()}"
    )

    # æ ¼å¼åŒ–éªŒè¯é”™è¯¯ä¿¡æ¯
    error_messages = []
    for error in exc.errors():
        field = " -> ".join(str(loc) for loc in error["loc"])
        message = error["msg"]
        error_messages.append(f"{field}: {message}")

    return JSONResponse(
        status_code=status.HTTP_400_BAD_REQUEST,
        content=ApiResponse(
            success=False,
            error=f"Validation error: {'; '.join(error_messages)}"
        ).dict()
    )


# æŒ‚è½½é™æ€æ–‡ä»¶ç›®å½•
app.mount("/static", StaticFiles(directory="static"), name="static")
# è®¾ç½®æ¨¡æ¿å¼•æ“
templates = Jinja2Templates(directory="templates")

# å†…å­˜ä¸­çš„ä»»åŠ¡è¿›åº¦è·Ÿè¸ªå­—å…¸
task_progress = {}


class PromptRequest(BaseModel):
    """è¯·æ±‚æ¨¡å‹ - ç”¨æˆ·æäº¤çš„ç ”ç©¶ä¸»é¢˜"""

    prompt: str


class QueueResearchTaskRequest(BaseModel):
    """ç ”ç©¶ä»»åŠ¡æ’é˜Ÿè¯·æ±‚æ¨¡å‹"""

    taskId: str
    prompt: Optional[str] = None
    model: Optional[str] = None


@app.get("/", response_class=HTMLResponse)
def read_index(request: Request):
    """
    é¦–é¡µè·¯ç”± - è¿”å› SSE æµå¼æ¥å£ç•Œé¢ï¼ˆPhase 2 æ ‡å‡†ï¼‰
    """
    return templates.TemplateResponse("index-sse.html", {"request": request})


@app.get("/legacy", response_class=HTMLResponse)
def read_legacy_index(request: Request):
    """
    æ—§ç‰ˆè½®è¯¢æ¥å£ç•Œé¢ï¼ˆå·²å¼ƒç”¨ï¼Œä¿ç•™ç”¨äºå…¼å®¹ï¼‰
    """
    return templates.TemplateResponse("index.html", {"request": request})


@app.get("/health", response_class=JSONResponse)
def health_check_legacy():
    """
    å¥åº·æ£€æŸ¥ç«¯ç‚¹ - ç”¨äºéªŒè¯æœåŠ¡æ˜¯å¦æ­£å¸¸è¿è¡Œ

    ã€ç®€åŒ–ç‰ˆæœ¬ã€‘ç”¨äºå¯åŠ¨è„šæœ¬å’Œå¿«é€Ÿå¥åº·æ£€æŸ¥
    è¯¦ç»†å¥åº·ä¿¡æ¯è¯·ä½¿ç”¨ /api/health
    """
    return {"status": "ok"}


@app.get("/api", response_class=JSONResponse)
def health_check(request: Request):
    """
    å¥åº·æ£€æŸ¥ç«¯ç‚¹ - ç”¨äºéªŒè¯æœåŠ¡æ˜¯å¦æ­£å¸¸è¿è¡Œ

    ã€å·²å¼ƒç”¨ã€‘æ­¤ç«¯ç‚¹ä¿ç•™ç”¨äºå‘åå…¼å®¹ï¼Œè¯·ä½¿ç”¨ /api/health
    """
    return {"status": "ok"}


@app.post("/api/research/tasks")
async def enqueue_research_task(request: QueueResearchTaskRequest):
    """
    å°†ç ”ç©¶ä»»åŠ¡åŠ å…¥åå°é˜Ÿåˆ—æ‰§è¡Œ
    """
    session = SessionLocal()
    try:
        task: Optional[ResearchTask] = (
            session.query(ResearchTask)
            .filter(ResearchTask.task_id == request.taskId)
            .one_or_none()
        )
        if not task:
            raise HTTPException(status_code=404, detail="Research task not found")

        prompt = request.prompt or task.topic
        if not prompt or not prompt.strip():
            raise HTTPException(status_code=400, detail="Prompt is required")

        previous_status = task.status
        task.topic = prompt
        task.status = "queued"
        task.report = None
        task.progress = default_progress()
        task.started_at = None
        task.completed_at = None
        task.failed_at = None

        queue_info = ensure_queue_info(task)
        previous_retries = queue_info.get("retryCount", 0)
        if not isinstance(previous_retries, int):
            try:
                previous_retries = int(previous_retries)  # type: ignore[arg-type]
            except Exception:
                previous_retries = 0
        queue_info.clear()
        now_iso = datetime.utcnow().isoformat() + "Z"
        queue_info["enqueuedAt"] = now_iso
        queue_info["retryCount"] = (
            previous_retries + 1 if previous_status in {"failed", "cancelled"} else previous_retries
        )
        task.queue_info = queue_info

        add_event(task, "queued", "Task queued for execution")
        session.commit()
    finally:
        session.close()

    research_task_queue.put(
        {"task_id": request.taskId, "prompt": prompt, "model": request.model}
    )
    logger.info(f"Task {request.taskId} enqueued for execution.")
    return {"taskId": request.taskId, "status": "queued"}


@app.get("/api/research/tasks/{task_id}")
async def get_research_task_status(task_id: str):
    """
    æŸ¥è¯¢ç ”ç©¶ä»»åŠ¡æœ€æ–°çŠ¶æ€
    """
    session = SessionLocal()
    try:
        task: Optional[ResearchTask] = (
            session.query(ResearchTask)
            .filter(ResearchTask.task_id == task_id)
            .one_or_none()
        )
        if not task:
            raise HTTPException(status_code=404, detail="Research task not found")

        progress = serialize_progress(task.progress)
        queue_info = serialize_queue_info(task.queue_info)
        created_at = (
            task.created_at.isoformat() + "Z" if task.created_at else None
        )
        updated_at = (
            task.updated_at.isoformat() + "Z" if task.updated_at else None
        )
        started_at = (
            task.started_at.isoformat() + "Z" if task.started_at else None
        )
        completed_at = (
            task.completed_at.isoformat() + "Z" if task.completed_at else None
        )
        failed_at = (
            task.failed_at.isoformat() + "Z" if task.failed_at else None
        )

        return {
            "taskId": task.task_id,
            "status": task.status,
            "topic": task.topic,
            "progress": progress,
            "report": task.report,
            "queueInfo": queue_info,
            "startedAt": started_at,
            "completedAt": completed_at,
            "failedAt": failed_at,
            "createdAt": created_at,
            "updatedAt": updated_at,
        }
    finally:
        session.close()


# === Phase 2: æ ‡å‡†åŒ– API æ¥å£ ===

@app.get("/api/health", response_model=ApiResponse)
async def health_check_v2():
    """
    å¥åº·æ£€æŸ¥ç«¯ç‚¹ï¼ˆPhase 2 æ ‡å‡†åŒ–ç‰ˆæœ¬ï¼‰

    è¿”å›æœåŠ¡çŠ¶æ€ä¿¡æ¯ï¼Œç”¨äºï¼š
    1. ç›‘æ§æœåŠ¡æ˜¯å¦æ­£å¸¸è¿è¡Œ
    2. é˜²æ­¢æœåŠ¡ä¼‘çœ ï¼ˆcron-job.org å®šæœŸ pingï¼‰
    3. è´Ÿè½½å‡è¡¡å™¨å¥åº·æ£€æŸ¥

    æ€§èƒ½è¦æ±‚ï¼š< 100ms

    è¿”å›ï¼š
        ApiResponse with HealthResponse data
        - status: "ok" | "degraded" | "error"
        - timestamp: ISO æ ¼å¼æ—¶é—´æˆ³
        - version: API ç‰ˆæœ¬å·

    çŠ¶æ€ç ï¼š
        200: æœåŠ¡æ­£å¸¸
        503: æœåŠ¡ä¸å¯ç”¨ï¼ˆä½†è¿›ç¨‹ä»åœ¨è¿è¡Œï¼‰
    """
    import time
    start_time = time.time()

    try:
        # æ£€æŸ¥æ•°æ®åº“è¿æ¥ï¼ˆå¯é€‰ï¼‰
        # å¦‚æœæ•°æ®åº“è¿æ¥å¤±è´¥ï¼Œä»ç„¶è¿”å› 200ï¼Œä½† status ä¸º "degraded"
        db_ok = True
        try:
            from sqlalchemy import text
            db = SessionLocal()
            db.execute(text("SELECT 1"))
            db.close()
        except Exception as e:
            logger.warning(f"âš ï¸ æ•°æ®åº“å¥åº·æ£€æŸ¥å¤±è´¥: {e}")
            db_ok = False

        # è®¡ç®—å“åº”æ—¶é—´
        elapsed_ms = (time.time() - start_time) * 1000

        # æ„å»ºå“åº”
        health_data = HealthResponse(
            status="ok" if db_ok else "degraded",
            timestamp=datetime.utcnow().isoformat() + "Z",
            version="2.0.0"
        )

        logger.info(f"âœ… å¥åº·æ£€æŸ¥æˆåŠŸ ({elapsed_ms:.1f}ms)")

        return ApiResponse(
            success=True,
            data=health_data.dict()
        )

    except Exception as e:
        logger.error(f"âŒ å¥åº·æ£€æŸ¥å¤±è´¥: {e}")
        return JSONResponse(
            status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
            content=ApiResponse(
                success=False,
                error="Service temporarily unavailable"
            ).dict()
        )


@app.get("/api/models", response_model=ApiResponse)
async def get_models():
    """
    è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨

    è¿”å›ç³»ç»Ÿä¸­é…ç½®çš„æ‰€æœ‰ AI æ¨¡å‹ä¿¡æ¯ï¼ŒåŒ…æ‹¬ï¼š
    - æ¨¡å‹åç§°å’Œæ ‡è¯†ç¬¦
    - æä¾›å•†ä¿¡æ¯
    - ä¸Šä¸‹æ–‡çª—å£å¤§å°
    - æ˜¯å¦æ”¯æŒæµå¼è¾“å‡º

    è¿™ä¸ªæ¥å£ï¼š
    1. ç”¨äºå‰ç«¯å±•ç¤ºå¯ç”¨æ¨¡å‹
    2. å¸®åŠ©ç”¨æˆ·äº†è§£ç³»ç»Ÿèƒ½åŠ›
    3. æ”¯æŒåŠ¨æ€æ¨¡å‹é€‰æ‹©ï¼ˆæœªæ¥åŠŸèƒ½ï¼‰

    æ€§èƒ½è¦æ±‚ï¼š< 200msï¼ˆé™æ€æ•°æ®ï¼‰

    è¿”å›ï¼š
        ApiResponse with list of ModelInfo

    ç¤ºä¾‹å“åº”ï¼š
        {
            "success": true,
            "data": {
                "models": [
                    {
                        "name": "deepseek:deepseek-chat",
                        "provider": "deepseek",
                        "description": "DeepSeek Chat - é«˜æ€§ä»·æ¯”é€šç”¨å¯¹è¯æ¨¡å‹",
                        "context_window": 65536,
                        "supports_streaming": true
                    },
                    ...
                ]
            }
        }
    """
    from src.config import ModelConfig

    try:
        # æ„å»ºæ¨¡å‹ä¿¡æ¯åˆ—è¡¨
        models = [
            ModelInfo(
                name=ModelConfig.PLANNER_MODEL,
                provider=ModelConfig.PLANNER_MODEL.split(":")[0],
                description="DeepSeek Reasoner - å¤æ‚æ¨ç†å’Œè§„åˆ’ä»»åŠ¡ä¸“ç”¨æ¨¡å‹",
                context_window=65536,
                supports_streaming=True
            ),
            ModelInfo(
                name=ModelConfig.RESEARCHER_MODEL,
                provider=ModelConfig.RESEARCHER_MODEL.split(":")[0],
                description="DeepSeek Chat - ç ”ç©¶å’Œä¿¡æ¯æœé›†",
                context_window=65536,
                supports_streaming=True
            ),
            ModelInfo(
                name=ModelConfig.WRITER_MODEL,
                provider=ModelConfig.WRITER_MODEL.split(":")[0],
                description="DeepSeek Chat - å†…å®¹ç”Ÿæˆå’Œå†™ä½œ",
                context_window=65536,
                supports_streaming=True
            ),
            ModelInfo(
                name=ModelConfig.EDITOR_MODEL,
                provider=ModelConfig.EDITOR_MODEL.split(":")[0],
                description="DeepSeek Chat - å†…å®¹å®¡æ ¸å’Œç¼–è¾‘",
                context_window=65536,
                supports_streaming=True
            ),
            ModelInfo(
                name=ModelConfig.FALLBACK_MODEL,
                provider=ModelConfig.FALLBACK_MODEL.split(":")[0],
                description="GPT-4O Mini - é«˜å¯é æ€§é™çº§æ¨¡å‹",
                context_window=128000,
                supports_streaming=True
            ),
        ]

        # å»é‡ï¼ˆç›¸åŒçš„æ¨¡å‹å¯èƒ½è¢«å¤šä¸ª agent ä½¿ç”¨ï¼‰
        unique_models = {}
        for model in models:
            if model.name not in unique_models:
                unique_models[model.name] = model

        logger.info(f"ğŸ“‹ è¿”å› {len(unique_models)} ä¸ªæ¨¡å‹ä¿¡æ¯")

        return ApiResponse(
            success=True,
            data={
                "models": [m.dict() for m in unique_models.values()],
                "total": len(unique_models),
                "default_model": ModelConfig.RESEARCHER_MODEL
            }
        )

    except Exception as e:
        logger.error(f"âŒ è·å–æ¨¡å‹åˆ—è¡¨å¤±è´¥: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to retrieve models: {str(e)}"
        )


@app.post("/generate_report")
def generate_report(req: PromptRequest):
    """
    ç”ŸæˆæŠ¥å‘Šç«¯ç‚¹ - æ¥æ”¶ç”¨æˆ·çš„ç ”ç©¶ä¸»é¢˜å¹¶å¯åŠ¨åå°ä»»åŠ¡
    
    å‚æ•°:
        req: åŒ…å«ç ”ç©¶ä¸»é¢˜çš„è¯·æ±‚å¯¹è±¡
    
    è¿”å›:
        åŒ…å«ä»»åŠ¡ ID çš„å­—å…¸
    """
    # ç”Ÿæˆå”¯ä¸€çš„ä»»åŠ¡ ID
    task_id = str(uuid.uuid4())
    
    # åœ¨æ•°æ®åº“ä¸­åˆ›å»ºä»»åŠ¡è®°å½•
    db = SessionLocal()
    db.add(Task(id=task_id, prompt=req.prompt, status="running"))
    db.commit()
    db.close()

    # åˆå§‹åŒ–ä»»åŠ¡è¿›åº¦è·Ÿè¸ª
    task_progress[task_id] = {"steps": []}
    
    # ä½¿ç”¨è§„åˆ’ä»£ç†ç”Ÿæˆæ‰§è¡Œæ­¥éª¤
    initial_plan_steps = planner_agent(req.prompt)
    
    # ä¸ºæ¯ä¸ªæ­¥éª¤åˆ›å»ºè¿›åº¦è·Ÿè¸ªæ¡ç›®
    for step_title in initial_plan_steps:
        task_progress[task_id]["steps"].append(
            {
                "title": step_title,
                "status": "pending",  # å¾…æ‰§è¡Œ
                "description": "ç­‰å¾…æ‰§è¡Œ",
                "substeps": [],
            }
        )

    # åœ¨åå°çº¿ç¨‹ä¸­å¯åŠ¨ä»£ç†å·¥ä½œæµ
    thread = threading.Thread(
        target=run_agent_workflow, args=(task_id, req.prompt, initial_plan_steps)
    )
    thread.start()
    return {"task_id": task_id}


@app.get("/task_progress/{task_id}")
def get_task_progress(task_id: str):
    """
    è·å–ä»»åŠ¡è¿›åº¦ç«¯ç‚¹ - è¿”å›ä»»åŠ¡çš„å®æ—¶æ‰§è¡Œè¿›åº¦
    
    å‚æ•°:
        task_id: ä»»åŠ¡å”¯ä¸€æ ‡è¯†ç¬¦
    
    è¿”å›:
        åŒ…å«æ­¥éª¤åˆ—è¡¨çš„è¿›åº¦ä¿¡æ¯
    """
    return task_progress.get(task_id, {"steps": []})


@app.get("/task_status/{task_id}")
def get_task_status(task_id: str):
    """
    è·å–ä»»åŠ¡çŠ¶æ€ç«¯ç‚¹ - è¿”å›ä»»åŠ¡çš„æœ€ç»ˆçŠ¶æ€å’Œç»“æœ
    
    å‚æ•°:
        task_id: ä»»åŠ¡å”¯ä¸€æ ‡è¯†ç¬¦
    
    è¿”å›:
        åŒ…å«ä»»åŠ¡çŠ¶æ€å’Œç»“æœçš„å­—å…¸
    """
    db = SessionLocal()
    task = db.query(Task).filter(Task.id == task_id).first()
    db.close()
    if not task:
        raise HTTPException(status_code=404, detail="ä»»åŠ¡æœªæ‰¾åˆ°")
    return {
        "status": task.status,
        "result": json.loads(task.result) if task.result else None,
    }


def format_history(history):
    """
    æ ¼å¼åŒ–æ‰§è¡Œå†å² - å°†å†å²è®°å½•è½¬æ¢ä¸ºå¯è¯»çš„å­—ç¬¦ä¸²æ ¼å¼
    
    å‚æ•°:
        history: æ‰§è¡Œå†å²åˆ—è¡¨ [(æ ‡é¢˜, æè¿°, è¾“å‡º), ...]
    
    è¿”å›:
        æ ¼å¼åŒ–çš„å†å²å­—ç¬¦ä¸²
    """
    return "\n\n".join(
        f"ğŸ”¹ {title}\n{desc}\n\nğŸ“ è¾“å‡º:\n{output}" for title, desc, output in history
    )


def run_agent_workflow(task_id: str, prompt: str, initial_plan_steps: list):
    """
    è¿è¡Œä»£ç†å·¥ä½œæµ - åœ¨åå°çº¿ç¨‹ä¸­æ‰§è¡Œå¤šä»£ç†ç ”ç©¶ä»»åŠ¡
    
    å‚æ•°:
        task_id: ä»»åŠ¡å”¯ä¸€æ ‡è¯†ç¬¦
        prompt: ç”¨æˆ·çš„ç ”ç©¶ä¸»é¢˜
        initial_plan_steps: è§„åˆ’ä»£ç†ç”Ÿæˆçš„æ‰§è¡Œæ­¥éª¤åˆ—è¡¨
    """
    steps_data = task_progress[task_id]["steps"]
    execution_history = []

    def update_step_status(index, status, description="", substep=None):
        """
        æ›´æ–°æ­¥éª¤çŠ¶æ€ - æ›´æ–°ä»»åŠ¡è¿›åº¦è·Ÿè¸ªä¸­çš„æ­¥éª¤ä¿¡æ¯
        
        å‚æ•°:
            index: æ­¥éª¤ç´¢å¼•
            status: æ–°çŠ¶æ€ï¼ˆpending, running, done, errorï¼‰
            description: çŠ¶æ€æè¿°
            substep: å­æ­¥éª¤ä¿¡æ¯ï¼ˆå¯é€‰ï¼‰
        """
        if index < len(steps_data):
            steps_data[index]["status"] = status
            if description:
                steps_data[index]["description"] = description
            if substep:
                steps_data[index]["substeps"].append(substep)
            steps_data[index]["updated_at"] = datetime.utcnow().isoformat()

    try:
        # éå†å¹¶æ‰§è¡Œæ¯ä¸ªè®¡åˆ’æ­¥éª¤
        for i, plan_step_title in enumerate(initial_plan_steps):
            update_step_status(i, "running", f"æ­£åœ¨æ‰§è¡Œ: {plan_step_title}")

            # æ‰§è¡Œå•ä¸ªä»£ç†æ­¥éª¤
            actual_step_description, agent_name, output = executor_agent_step(
                plan_step_title, execution_history, prompt
            )

            # å°†æ‰§è¡Œç»“æœæ·»åŠ åˆ°å†å²è®°å½•
            execution_history.append([plan_step_title, actual_step_description, output])

            # HTML è½¬ä¹‰å‡½æ•°
            def esc(s: str) -> str:
                return html.escape(s or "")

            def nl2br(s: str) -> str:
                return esc(s).replace("\n", "<br>")

            # æ›´æ–°æ­¥éª¤çŠ¶æ€ä¸ºå®Œæˆï¼Œå¹¶æ·»åŠ è¯¦ç»†çš„æ‰§è¡Œä¿¡æ¯
            update_step_status(
                i,
                "done",
                f"å·²å®Œæˆ: {plan_step_title}",
                {
                    "title": f"è°ƒç”¨äº† {agent_name}",
                    "content": f"""
<div style='border:1px solid #ccc; border-radius:8px; padding:10px; margin:8px 0; background:#fff;'>
  <div style='font-weight:bold; color:#2563eb;'>ğŸ“˜ ç”¨æˆ·æç¤º</div>
  <div style='white-space:pre-wrap;'>{prompt}</div>

  <div style='font-weight:bold; color:#16a34a; margin-top:8px;'>ğŸ“œ ä¸Šä¸€æ­¥</div>
  <pre style='white-space:pre-wrap; background:#f9fafb; padding:6px; border-radius:6px; margin:0;'>
{format_history(execution_history[-2:-1])}
  </pre>

  <div style='font-weight:bold; color:#f59e0b; margin-top:8px;'>ğŸ§¹ å½“å‰ä»»åŠ¡</div>
  <div style='white-space:pre-wrap;'>{actual_step_description}</div>

  <div style='font-weight:bold; color:#10b981; margin-top:8px;'>âœ… è¾“å‡º</div>
  <!-- âš ï¸ è¿™é‡Œä¸ä½¿ç”¨ <pre> æ ‡ç­¾ä»¥ä¿ç•™ HTML æ ¼å¼ -->
  <div style='white-space:pre-wrap;'>
{output}
  </div>
</div>
""".strip(),
                },
            )

        # æå–æœ€ç»ˆæŠ¥å‘Šï¼ˆæœ€åä¸€æ­¥çš„è¾“å‡ºï¼‰
        final_report_markdown = (
            execution_history[-1][-1] if execution_history else "æœªç”ŸæˆæŠ¥å‘Šã€‚"
        )

        # æ„å»ºç»“æœå¯¹è±¡
        result = {"html_report": final_report_markdown, "history": steps_data}

        # æ›´æ–°æ•°æ®åº“ä¸­çš„ä»»åŠ¡çŠ¶æ€
        db = SessionLocal()
        task = db.query(Task).filter(Task.id == task_id).first()
        task.status = "done"
        task.result = json.dumps(result)
        task.updated_at = datetime.utcnow()
        db.commit()
        db.close()

    except Exception as e:
        # å¤„ç†å·¥ä½œæµæ‰§è¡Œè¿‡ç¨‹ä¸­çš„é”™è¯¯
        print(f"ä»»åŠ¡ {task_id} çš„å·¥ä½œæµé”™è¯¯: {e}")
        
        # æ‰¾åˆ°å‡ºé”™çš„æ­¥éª¤å¹¶æ›´æ–°å…¶çŠ¶æ€
        if steps_data:
            error_step_index = next(
                (i for i, s in enumerate(steps_data) if s["status"] == "running"),
                len(steps_data) - 1,
            )
            if error_step_index >= 0:
                update_step_status(
                    error_step_index,
                    "error",
                    f"æ‰§è¡Œè¿‡ç¨‹ä¸­å‡ºé”™: {e}",
                    {"title": "é”™è¯¯", "content": str(e)},
                )

        # æ›´æ–°æ•°æ®åº“ä¸­çš„ä»»åŠ¡çŠ¶æ€ä¸ºé”™è¯¯
        db = SessionLocal()
        task = db.query(Task).filter(Task.id == task_id).first()
        task.status = "error"
        task.updated_at = datetime.utcnow()
        db.commit()
        db.close()


# === Phase 2: SSE æµå¼æ¥å£ ===

@app.post("/api/research/stream")
async def research_stream(request: ResearchRequest):
    """
    SSE æµå¼ç ”ç©¶æ¥å£ï¼ˆPhase 2 æ ¸å¿ƒåŠŸèƒ½ï¼‰

    æ¥æ”¶ç ”ç©¶ä¸»é¢˜ï¼Œé€šè¿‡ Server-Sent Events (SSE) å®æ—¶æ¨é€ç ”ç©¶è¿›åº¦å’Œç»“æœã€‚

    å·¥ä½œæµç¨‹ï¼š
        1. å‘é€ START äº‹ä»¶ï¼ˆåŒ…å« promptï¼‰
        2. è°ƒç”¨ planner_agent ç”Ÿæˆæ‰§è¡Œè®¡åˆ’
        3. å‘é€ PLAN äº‹ä»¶ï¼ˆåŒ…å«æ­¥éª¤åˆ—è¡¨ï¼‰
        4. å¾ªç¯æ‰§è¡Œæ¯ä¸ªæ­¥éª¤ï¼š
           a. å‘é€ PROGRESS äº‹ä»¶ï¼ˆæ­¥éª¤ç¼–å·ã€æ€»æ•°ã€æè¿°ï¼‰
           b. è°ƒç”¨ executor_agent_step æ‰§è¡Œæ­¥éª¤
        5. å‘é€ DONE äº‹ä»¶ï¼ˆåŒ…å«æœ€ç»ˆæŠ¥å‘Šï¼‰
        6. å¦‚æœå‡ºé”™ï¼Œå‘é€ ERROR äº‹ä»¶

    SSE äº‹ä»¶ç±»å‹ï¼š
        - start: ä»»åŠ¡å¼€å§‹ï¼Œdata: {prompt}
        - plan: æ‰§è¡Œè®¡åˆ’ï¼Œdata: {steps: [str]}
        - progress: æ­¥éª¤è¿›åº¦ï¼Œdata: {step, total, message}
        - done: ä»»åŠ¡å®Œæˆï¼Œdata: {report: str}
        - error: å‘ç”Ÿé”™è¯¯ï¼Œdata: {message, step?}

    å‚æ•°ï¼š
        request: ResearchRequest
            - prompt: ç ”ç©¶ä¸»é¢˜ï¼ˆå¿…éœ€ï¼Œ10-5000 å­—ç¬¦ï¼‰
            - model: å¯é€‰çš„æ¨¡å‹åç§°

    è¿”å›ï¼š
        StreamingResponse (text/event-stream)

    å“åº”å¤´ï¼š
        - Content-Type: text/event-stream
        - Cache-Control: no-cache, no-transform
        - Connection: keep-alive
        - X-Accel-Buffering: no (ç¦ç”¨ Nginx ç¼“å†²)

    æ€§èƒ½è¦æ±‚ï¼š
        - é¦–ä¸ªäº‹ä»¶ (START) < 2s
        - å®Œæ•´ä»»åŠ¡ < 5min
        - æ”¯æŒ 5 ä¸ªå¹¶å‘è¿æ¥

    é”™è¯¯å¤„ç†ï¼š
        - æ‰€æœ‰å¼‚å¸¸éƒ½ä¼šå‘é€ ERROR äº‹ä»¶
        - ä¸ä¼šä¸­æ–­è¿æ¥ï¼ˆå®¢æˆ·ç«¯å†³å®šä½•æ—¶å…³é—­ï¼‰
        - è¯¦ç»†é”™è¯¯è®°å½•åˆ°æ—¥å¿—

    ä½¿ç”¨ç¤ºä¾‹ï¼ˆcurlï¼‰ï¼š
        curl -X POST http://localhost:8000/api/research/stream \\
             -H "Content-Type: application/json" \\
             -d '{"prompt": "Research AI applications"}' \\
             -N

    ä½¿ç”¨ç¤ºä¾‹ï¼ˆJavaScriptï¼‰ï¼š
        const eventSource = new EventSource('/api/research/stream');
        eventSource.addEventListener('start', (e) => {
            const data = JSON.parse(e.data);
            console.log('Started:', data.prompt);
        });
        eventSource.addEventListener('progress', (e) => {
            const data = JSON.parse(e.data);
            console.log(`Step ${data.step}/${data.total}: ${data.message}`);
        });
        eventSource.addEventListener('done', (e) => {
            const data = JSON.parse(e.data);
            console.log('Report:', data.report);
            eventSource.close();
        });

    è®¾è®¡å†³ç­–ï¼š
        - ä¸ä½¿ç”¨æ•°æ®åº“ï¼ˆå‡å°‘å»¶è¿Ÿï¼Œç¬¦åˆ MVP åŸåˆ™ï¼‰
        - ä¸ä¿å­˜å†å²ï¼ˆå®¢æˆ·ç«¯è´Ÿè´£è®°å½•ï¼‰
        - ä½¿ç”¨å¼‚æ­¥ç”Ÿæˆå™¨ï¼ˆasync generatorï¼‰
        - é”™è¯¯æ—¶ä¸å…³é—­è¿æ¥ï¼ˆå®¢æˆ·ç«¯æ§åˆ¶ï¼‰
    """
    logger.info(f"ğŸš€ SSE æµå¼ç ”ç©¶è¯·æ±‚: {request.prompt[:50]}...")

    async def event_generator():
        """
        å¼‚æ­¥äº‹ä»¶ç”Ÿæˆå™¨

        ä½¿ç”¨ async generator é€æ­¥ç”Ÿæˆ SSE äº‹ä»¶ã€‚
        è¿™å…è®¸ï¼š
        1. éé˜»å¡æ‰§è¡Œ
        2. å®æ—¶æ¨é€äº‹ä»¶
        3. è‡ªåŠ¨æ¸…ç†èµ„æº
        """
        try:
            # === 1. START äº‹ä»¶ ===
            logger.info("ğŸ“¤ å‘é€ START äº‹ä»¶")
            yield create_start_event(request.prompt)

            # === 2. PLAN äº‹ä»¶ - è°ƒç”¨ planner_agent ===
            logger.info("ğŸ§  è°ƒç”¨ planner_agent ç”Ÿæˆæ‰§è¡Œè®¡åˆ’")
            try:
                steps = planner_agent(
                    request.prompt,
                    model=request.model
                )
                logger.info(f"âœ… ç”Ÿæˆäº† {len(steps)} ä¸ªæ‰§è¡Œæ­¥éª¤")
            except Exception as e:
                logger.error(f"âŒ planner_agent å¤±è´¥: {e}")
                yield create_error_event(f"Failed to generate plan: {str(e)}")
                return

            logger.info("ğŸ“¤ å‘é€ PLAN äº‹ä»¶")
            yield create_plan_event(steps)

            # === 3. æ‰§è¡Œæ­¥éª¤å¾ªç¯ ===
            execution_history = []

            for i, step_title in enumerate(steps):
                step_number = i + 1
                logger.info(f"ğŸ“¤ å‘é€ PROGRESS äº‹ä»¶: {step_number}/{len(steps)}")
                yield create_progress_event(
                    step=step_number,
                    total=len(steps),
                    message=step_title
                )

                logger.info(f"âš™ï¸  æ‰§è¡Œæ­¥éª¤ {step_number}: {step_title[:50]}...")
                try:
                    step_desc, agent_name, output = executor_agent_step(
                        step_title,
                        execution_history,
                        request.prompt
                    )
                    execution_history.append([step_title, step_desc, output])
                    logger.info(
                        f"âœ… æ­¥éª¤ {step_number} å®Œæˆï¼Œ"
                        f"ä½¿ç”¨ä»£ç†: {agent_name}ï¼Œ"
                        f"è¾“å‡ºé•¿åº¦: {len(output)} å­—ç¬¦"
                    )
                except Exception as e:
                    logger.error(f"âŒ æ­¥éª¤ {step_number} æ‰§è¡Œå¤±è´¥: {e}")
                    yield create_error_event(
                        message=f"Step {step_number} failed: {str(e)}",
                        step=step_number
                    )
                    return

            if execution_history:
                final_report = execution_history[-1][2]
                logger.info(f"ğŸ“¤ å‘é€ DONE äº‹ä»¶ï¼ŒæŠ¥å‘Šé•¿åº¦: {len(final_report)} å­—ç¬¦")
                yield create_done_event(final_report)
            else:
                logger.warning("âš ï¸ æ‰§è¡Œå†å²ä¸ºç©ºï¼Œæ— æ³•ç”ŸæˆæŠ¥å‘Š")
                yield create_error_event("No report generated")

        except Exception as e:
            logger.error(f"âŒ SSE ç”Ÿæˆå™¨å‘ç”Ÿæœªå¤„ç†çš„å¼‚å¸¸: {e}\n{traceback.format_exc()}")
            yield create_error_event(f"Internal error: {str(e)}")

    # è¿”å› StreamingResponse
    return StreamingResponse(
        event_generator(),
        media_type="text/event-stream",
        headers=get_sse_headers()
    )
