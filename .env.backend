# ============================================================================
# FastAPI Backend Environment Variables
# Phase 4 Deployment Configuration
# ============================================================================

# ----------------------------------------------------------------------------
# API Keys (Required)
# ----------------------------------------------------------------------------

# DeepSeek API - Primary model provider (~45% cost savings vs OpenAI)
DEEPSEEK_API_KEY=your-deepseek-key-here

# OpenAI API - Fallback model provider
OPENAI_API_KEY=your-openai-key-here

# Tavily API - Search tool provider
TAVILY_API_KEY=your-tavily-key-here

# Serper API - Alternative search tool (optional)
SERPER_API_KEY=your-serper-key-here

# ----------------------------------------------------------------------------
# Database Configuration
# ----------------------------------------------------------------------------

# PostgreSQL connection string
# Local development: postgresql://postgres:postgres@localhost:5432/ai_research
# Docker: postgresql://postgres:postgres@postgres:5432/ai_research
# Neon (production): postgresql://user:pass@ep-xxx.neon.tech/research?sslmode=require
DATABASE_URL=postgresql://postgres:postgres@localhost:5432/ai_research

# ----------------------------------------------------------------------------
# Server Configuration
# ----------------------------------------------------------------------------

# Host address (0.0.0.0 for Docker, 127.0.0.1 for local)
HOST=0.0.0.0

# Server port
PORT=8000

# Number of workers (1 for development, 2-4 for production)
WORKERS=1

# Log level (DEBUG, INFO, WARNING, ERROR)
LOG_LEVEL=INFO

# ----------------------------------------------------------------------------
# CORS Configuration
# ----------------------------------------------------------------------------

# Allowed origins (comma-separated)
# Local: http://localhost:3000
# Production: https://your-app.vercel.app
ALLOWED_ORIGINS=http://localhost:3000,http://127.0.0.1:3000

# ----------------------------------------------------------------------------
# Model Configuration
# ----------------------------------------------------------------------------

# Planner Agent - Strong reasoning capability
PLANNER_MODEL=deepseek:deepseek-reasoner

# Researcher Agent - Tool calling compatibility
RESEARCHER_MODEL=deepseek:deepseek-chat

# Writer Agent - Long text generation
WRITER_MODEL=deepseek:deepseek-chat

# Editor Agent - Text refinement
EDITOR_MODEL=deepseek:deepseek-chat

# Fallback Model - Auto-switch on failure
FALLBACK_MODEL=openai:gpt-4o-mini

# ----------------------------------------------------------------------------
# Context Management (Phase 1.5)
# ----------------------------------------------------------------------------

# Enable chunking for long texts
ENABLE_CHUNKING=true

# Chunking threshold (trigger when > 80% of context window)
CHUNKING_THRESHOLD=0.8

# Maximum chunk size (tokens)
MAX_CHUNK_SIZE=6000

# Chunk overlap size (tokens)
CHUNK_OVERLAP=200

# ----------------------------------------------------------------------------
# Feature Flags
# ----------------------------------------------------------------------------

# Enable cost tracking
ENABLE_COST_TRACKING=true

# Enable fallback mechanism
ENABLE_FALLBACK=true

# ----------------------------------------------------------------------------
# Production Settings (Render)
# ----------------------------------------------------------------------------

# Environment name
ENV=development

# Render service URL (set by Render automatically in production)
# RENDER_EXTERNAL_URL=https://your-app.onrender.com
